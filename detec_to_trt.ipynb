{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4e3105",
   "metadata": {},
   "source": [
    "## Change the default torch version for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc0193",
   "metadata": {},
   "source": [
    "## Clone Detectron2 repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e7f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/detectron2.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f65b027",
   "metadata": {},
   "source": [
    "## Resize the sample image to required square shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc48db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "I1= cv2.imread('sample_image.jpg')\n",
    "\n",
    "# change 960 to what the model has been trained on\n",
    "I2 = cv2.resize(I1, (960, 960))\n",
    "\n",
    "print(I2.shape)\n",
    "cv2.imwrite('new.jpg', I2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2635492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(I2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40a9c18",
   "metadata": {},
   "source": [
    "## Export model to Caffe2 format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438461b",
   "metadata": {},
   "source": [
    "Make sure to edit **export_model.py**, change these sizes to what model has been trained on, as above for sample_image.\n",
    "\n",
    "<code>aug = T.ResizeShortestEdge(\n",
    "    [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    ")</code>\n",
    "\n",
    "to\n",
    "\n",
    "<code>aug = T.ResizeShortestEdge(\n",
    "    [960, 960], 960\n",
    ")</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "825dfba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#files required:\n",
    "# model config.yaml\n",
    "# model weights .pth file\n",
    "# sample resized image\n",
    "\n",
    "!python detectron2/tools/deploy/export_model.py  --config-file config.yaml --output ./ --format onnx --sample-image new.jpg --export-method caffe2_tracing MODEL.DEVICE cuda MODEL.WEIGHTS model.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ab646",
   "metadata": {},
   "source": [
    "## Convert Caffe2 to Onnx format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae1884",
   "metadata": {},
   "source": [
    "Make sure to edit **create_onnx.py** file\n",
    "\n",
    "In **get_anchors** function:\n",
    "1. change *image preprocessing* to your model's image preprocessing\n",
    "2. do the same for *model preprocessing* if any\n",
    "3. change size (960 in this case) on line: <code>imagelist_images = ImageList.from_tensors(images, 1344)</code>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "410d0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#files required:\n",
    "# model config.yaml\n",
    "# model weights .pth file\n",
    "# sample resized image\n",
    "# caffe2 format model from previous step (model.onnx)\n",
    "\n",
    "!python create_onnx.py --onnx converted.onnx --exported_onnx model.onnx --det2_config config.yaml -s new.jpg --det2_weights model.pth "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde82692",
   "metadata": {},
   "source": [
    "## Create TensorRT engine from converted Onnx model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43f87d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#files required:\n",
    "# onnx file from previous step (converted.onnx)\n",
    "\n",
    "!trtexec --onnx=converted.onnx --saveEngine=engine.trt --useCudaGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d753e",
   "metadata": {},
   "source": [
    "## Optional: Run TensorRT engine to analyze performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b39cf1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!trtexec --loadEngine=engine.trt --useCudaGraph --noDataTransfers --iterations=100 --avgRuns=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b4bc5d",
   "metadata": {},
   "source": [
    "## Optional: Run inference on sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf46f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to change labels in this file, but not necessary if only checking the masks\n",
    "\n",
    "!python infer.py --engine engine.trt --input new.jpg --det2_config config.yml --output output/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
